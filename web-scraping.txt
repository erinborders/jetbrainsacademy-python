Using library Scrapy

WEBSCRAPING PIPELINE

1. setup - define objective and find sources to help us do it
2. acquisition - accessing, parsing, and extracting data into data structures
3. processing - running data through processes to achieve desired goal

WRITING MULTILINE VARIABLES IN PYTHON

html = '''
<html>
  <head>
    <title>Intro HTML</title>
  </head>
  <body>
    <p>Hello World!</p>
  </body>
</html>
'''

X PATH NOTATION

xpath = '/html/body/div[2]'

// means looking through all future generations
xpath = '/html/body/div[2]//table 
means navigate to all table elements that are a descendant of that
div[2] elements

xpath = '//p'
mean navigate to all p elements

xpath = '//div[@id="uid"]'
means navigate to all div elements with the id 'uid'

xpath is general, not python specific

BRAD TRAVERSY WEB SCRAPING TUTORIAL

1. open vscode and create python virtual env (python3 -m venv <folder name>)
(FOR ME = use miniconda shell with command: conda create --name myenv)
2. once you run that in terminal there will be a <folder name> in vscode, and inside
that will be a bin folder that contains the activate script - so you want
to run that (source <folder>/bin/activate)
(FOR ME = source Scripts/activate)
3. ctrl + shift + p then search for python, select interpreter then select
your virtual env
4. install scrapy with pip 
(FOR ME = conda install -c conda-forge scrapy)
5. run scrapy startproject <project name> and cd into project 
(the middleware folder is for prepping data from response, like cleaning it, validating it, etc)
6. inside spider folder create a file (this will be your main file)
7. import scrapy 
8. create spider class that extends scrapy.spider 
9. use kite extension to see what scrapy.spider object expects

class MySpider(scrapy.Spider):
    name = 'posts'

    start_urls = [ // this is the shorthand for start requests function
        'example url/1/',
        'example url/2/'
    ]

    // use parse - it's in charge of the res when returning the scraped data
    def parse(self, response):
        page = response.url.split('/)[-1] // will give us the slash piece one from the end (the page number in this case)
        filename = 'posts-%s.html' % page // the %s is a wildcard and the % page tells the computer what we're substituting in
        with open(filename, 'wb') as f: // wb is write binary permissions
            f.write(response.body)

Then you can run the above with 'scrapy crawl <name of spider, which would be posts>' in terminal 


Go into scrapy shell using 'scrapy shell <url that we want to crawl>' in terminal 
 --> then you can manipulate the response 
    EXAMPLE: response.css('h3::text').get() 

TO USE XPATH SELECTORS
response.xpath('//h3')
response.xpath('//h3/text()').extract() // gets you all the text in the h3's 

ON CHROME there's an option to copy the xpath if you right click on an 
element inside the developer tools elements tab 

TO GO THROUGH PAGES WITH SPIDER 

    def parse(self, response):
        for post in response.css('div.post-item'):
            yield {
                'title': post.css('.post-header h2 a::text')[0].get()
                'author': post.css('.post-header a::text')[1].get()
            }

        // find pagination
        next_page = response.css('a.next-posts-link::attr(href)').get()
        if next_page is not None: // if next page exists
            next_page = response.urljoin(next_page) // join in next page so that it scrapes that as well 
            yield scrapy.Request(next_page, callback=self.parse)

TO PUT INTO A JSON FILE USE -O OUTPUT FLAG
(scrapy crawl posts -o posts.json) // choose json list to keep new entries from overwriting entire file 
// doesn't overwrite just because you're scraping a bunch of pages 

PYTHON SCRAPY TUTORIAL - KITE CHANNEL

// items are the data extracted from selectors 
// scrapy item classes define how our scraped data should be structured 

EXAMPLE:
class Article(scrapy.Item):
    headline = scrapy.Field()

to get images
response.xpath("//img/@src")

RUN THE SPIDER BY NAVIGATING TO MAIN DIRECTORY IN CONSOLE THEN 
ISSUING COMMAND 'SCRAPY CRAWL <SPIDER NAME>'


